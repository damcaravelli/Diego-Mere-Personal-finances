{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0541493",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "403c55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173eaabb",
   "metadata": {},
   "source": [
    "## Directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efd36fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Project structure validated/created\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# Project structure and paths (portable)\n",
    "# =======================================================\n",
    "\n",
    "# If running inside a folder named \"notebooks\" (case-insensitive), use its parent as ROOT.\n",
    "cwd = Path.cwd()\n",
    "ROOT = cwd.parents[0] if cwd.name.lower() == \"notebooks\" else cwd\n",
    "\n",
    "RAW_DIR       = ROOT / \"data\" / \"raw\"\n",
    "STAGE_CSV_DIR = ROOT / \"data\" / \"stage_csv\"\n",
    "PROCESSED_DIR = ROOT / \"data\" / \"processed\"\n",
    "RULES_DIR     = ROOT / \"rules\"\n",
    "\n",
    "combined_csv_path = PROCESSED_DIR / \"combined.csv\"\n",
    "dictionary_path   = RULES_DIR / \"categories_dictionary.csv\"\n",
    "final_output_path = PROCESSED_DIR / \"categorized_expenses.csv\"\n",
    "\n",
    "for folder in [RAW_DIR, STAGE_CSV_DIR, PROCESSED_DIR, RULES_DIR]:\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"✅ Project structure validated/created\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea199fd",
   "metadata": {},
   "source": [
    "## Manipulacion de archivos y conexion a datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4661f8ed",
   "metadata": {},
   "source": [
    "### Funciones "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4216660",
   "metadata": {},
   "source": [
    "#### Funcion para extraer datos de los extractos en excel originales \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0860e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_excel(file_path, max_header_rows=5):\n",
    "    \"\"\"\n",
    "    Process a bank Excel file and return a formatted DataFrame.\n",
    "    It automatically detects the header row and normalizes column names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Helper: normalize column names (lowercase, no accents, no dots)\n",
    "    def normalize_columns(cols):\n",
    "        def strip_accents(s):\n",
    "            return \"\".join(ch for ch in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(ch))\n",
    "        out = []\n",
    "        for c in cols:\n",
    "            c = strip_accents(str(c)).strip().lower()\n",
    "            c = c.replace(\".\", \"\").replace(\"  \", \" \")\n",
    "            out.append(c)\n",
    "        return out\n",
    "\n",
    "    # 1) Detect the header row within the first few rows\n",
    "    preview = pd.read_excel(file_path, header=None, nrows=max_header_rows)\n",
    "    keywords = {\"fecha\", \"concepto\", \"importe\", \"divisa\", \"movimiento\"}\n",
    "    header_row = 0\n",
    "    for i in range(len(preview)):\n",
    "        cols = normalize_columns(preview.iloc[i].tolist())\n",
    "        if any(k in cols for k in keywords):\n",
    "            header_row = i\n",
    "            break\n",
    "\n",
    "    # 2) Read again with the detected header row\n",
    "    df = pd.read_excel(file_path, header=header_row)\n",
    "    df = df.dropna(how=\"all\")  # drop completely empty rows\n",
    "\n",
    "    # 3) Normalize column names\n",
    "    df.columns = normalize_columns(df.columns)\n",
    "\n",
    "    # 4) Map possible aliases to standard column names\n",
    "    column_aliases = {\n",
    "        \"f_valor\": [\"f valor\", \"fecha valor\"],\n",
    "        \"fecha\": [\"fecha\", \"f operacion\"],\n",
    "        \"concepto\": [\"concepto\", \"detalle\", \"descripcion\", \"descripción\"],\n",
    "        \"movimiento\": [\"movimiento\", \"tipo\", \"operacion\"],\n",
    "        \"importe\": [\"importe\", \"monto\", \"cantidad\"],\n",
    "        \"divisa\": [\"divisa\", \"moneda\"],\n",
    "        \"disponible\": [\"disponible\", \"saldo\"],\n",
    "        \"observaciones\": [\"observaciones\", \"notas\", \"comentarios\"],\n",
    "    }\n",
    "\n",
    "    mapping = {}\n",
    "    for std, aliases in column_aliases.items():\n",
    "        for a in aliases:\n",
    "            if a in df.columns:\n",
    "                mapping[std] = a\n",
    "                break\n",
    "\n",
    "    if not mapping:\n",
    "        raise ValueError(f\"No standard columns recognized in {file_path}\")\n",
    "\n",
    "    df = df.rename(columns=mapping)\n",
    "\n",
    "    # 5) Reorder columns if they exist\n",
    "    ordered_cols = [\"f_valor\", \"fecha\", \"concepto\", \"movimiento\", \"importe\", \"divisa\", \"disponible\", \"observaciones\"]\n",
    "    final_cols = [c for c in ordered_cols if c in df.columns] + [c for c in df.columns if c not in ordered_cols]\n",
    "\n",
    "    return df[final_cols].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd62ca",
   "metadata": {},
   "source": [
    "#### Funcion para converitir los df resultantes a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8be781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv(df, csv_path):\n",
    "    \"\"\"\n",
    "    Save a DataFrame as a CSV file.\n",
    "    - Creates the parent folder if it does not exist.\n",
    "    - Uses utf-8-sig encoding so Excel can open it with accents.\n",
    "    \"\"\"\n",
    "    csv_path = Path(csv_path)\n",
    "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0addb246",
   "metadata": {},
   "source": [
    "#### Funcion para extraer los documentos de excel del directorio original para guardarlos como CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd2ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_excels(input_dir, output_dir, skip_existing=True):\n",
    "    \"\"\"\n",
    "    Convert all .xlsx files in input_dir to CSVs in output_dir.\n",
    "    - Skips temporary Excel files (~$...)\n",
    "    - Creates output folder if it does not exist\n",
    "    - If skip_existing=True, does not reprocess files that already have a CSV\n",
    "    - Uses process_excel(...) and save_as_csv(...)\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    xlsx_files = sorted(p for p in input_dir.glob(\"*.xlsx\") if not p.name.startswith(\"~$\"))\n",
    "    if not xlsx_files:\n",
    "        print(f\"No .xlsx files found in: {input_dir}\")\n",
    "        return []\n",
    "\n",
    "    converted_paths = []\n",
    "    for xlsx_path in xlsx_files:\n",
    "        csv_path = output_dir / (xlsx_path.stem + \".csv\")\n",
    "\n",
    "        if skip_existing and csv_path.exists():\n",
    "            print(f\"↷ Skip (already exists): {csv_path.name}\")\n",
    "            converted_paths.append(csv_path)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            df = process_excel(xlsx_path)\n",
    "            save_as_csv(df, csv_path)\n",
    "            print(f\"✔ {xlsx_path.name} → {csv_path.name} ({len(df)} rows)\")\n",
    "            converted_paths.append(csv_path)\n",
    "        except Exception as e:\n",
    "            print(f\"✖ Error processing {xlsx_path.name}: {e}\")\n",
    "\n",
    "    return converted_paths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d4854d",
   "metadata": {},
   "source": [
    "#### Funcion para leer todos los CSV del directorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f858e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_files(directory):\n",
    "    \"\"\"\n",
    "    Read all CSV files in a directory and return them as a list of DataFrames.\n",
    "    - Skips files that cannot be read\n",
    "    - Prints status messages\n",
    "    \"\"\"\n",
    "    directory = Path(directory)\n",
    "    csv_files = sorted(directory.glob(\"*.csv\"))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in: {directory}\")\n",
    "        return []\n",
    "\n",
    "    dataframes = []\n",
    "    for csv_path in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_path)\n",
    "            dataframes.append(df)\n",
    "            print(f\"✔ Loaded {csv_path.name} ({len(df)} rows)\")\n",
    "        except Exception as e:\n",
    "            print(f\"✖ Error reading {csv_path.name}: {e}\")\n",
    "\n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6550e9a",
   "metadata": {},
   "source": [
    "#### Funcion para combinar CSV en un solo dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44200c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(dataframes, drop_duplicates=True):\n",
    "    \"\"\"\n",
    "    Combine a list of DataFrames into a single DataFrame.\n",
    "    - Returns an empty DataFrame if the list is empty\n",
    "    - Optionally drops duplicate rows\n",
    "    \"\"\"\n",
    "    if not dataframes:\n",
    "        print(\"⚠ No DataFrames to combine. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    if drop_duplicates:\n",
    "        before = len(combined)\n",
    "        combined = combined.drop_duplicates()\n",
    "        after = len(combined)\n",
    "        if before != after:\n",
    "            print(f\"⚠ Removed {before - after} duplicate rows\")\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec590b14",
   "metadata": {},
   "source": [
    "#### Funciones  maestra para convertir CSV en un CSV final para trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab3d44a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_combined_files(input_dir=STAGE_CSV_DIR, output_path=combined_csv_path):\n",
    "    \"\"\"\n",
    "    Read all intermediate CSVs from input_dir, combine them into a single DataFrame,\n",
    "    optionally de-duplicate on common key columns if available, and save to output_path.\n",
    "    Returns the combined DataFrame (or None if nothing to combine).\n",
    "    \"\"\"\n",
    "    # 1) Load all CSVs in the staging folder\n",
    "    dataframes = read_csv_files(input_dir)\n",
    "    if not dataframes:\n",
    "        print(f\"No CSV files found in: {input_dir}\")\n",
    "        return None\n",
    "\n",
    "    # 2) Combine them (and drop exact duplicate rows)\n",
    "    df_combined = combine_dataframes(dataframes, drop_duplicates=True)\n",
    "\n",
    "    # 3) Optional: de-duplicate on typical bank keys if those columns exist\n",
    "    key_cols = [c for c in [\"fecha\", \"concepto\", \"importe\", \"divisa\"] if c in df_combined.columns]\n",
    "    if key_cols:\n",
    "        before = len(df_combined)\n",
    "        df_combined = df_combined.drop_duplicates(subset=key_cols)\n",
    "        removed = before - len(df_combined)\n",
    "        if removed > 0:\n",
    "            print(f\"⚠ Removed {removed} duplicates using keys: {key_cols}\")\n",
    "\n",
    "    # 4) Save combined CSV (UTF-8 with BOM so Excel reads accents correctly)\n",
    "    save_as_csv(df_combined, output_path)\n",
    "    print(f\"✅ Combined file saved to: {output_path} ({len(df_combined)} rows)\")\n",
    "\n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d84875f",
   "metadata": {},
   "source": [
    "### Proceso de extraccion de archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f6b727",
   "metadata": {},
   "source": [
    "#### Conversion de .xlsx a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffe82827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Abril-Junio 2024.xlsx → Abril-Junio 2024.csv (334 rows)\n",
      "✔ Agosto-Diciembre 2021.xlsx → Agosto-Diciembre 2021.csv (406 rows)\n",
      "✔ Diciembre-Agosto 2025.xlsx → Diciembre-Agosto 2025.csv (506 rows)\n",
      "✔ Diciembre-Febrero 2025.xlsx → Diciembre-Febrero 2025.csv (220 rows)\n",
      "✔ Enero - Abril 2023.xlsx → Enero - Abril 2023.csv (334 rows)\n",
      "✔ Enero-Abril 2022.xlsx → Enero-Abril 2022.csv (275 rows)\n",
      "✔ Enero-Marzo 2024.xlsx → Enero-Marzo 2024.csv (294 rows)\n",
      "✔ Enero-Mayo 2020.xlsx → Enero-Mayo 2020.csv (236 rows)\n",
      "✔ Julio-Agosto 2024.xlsx → Julio-Agosto 2024.csv (353 rows)\n",
      "✔ Junio-Diciembre 2019.xlsx → Junio-Diciembre 2019.csv (349 rows)\n",
      "✔ Junio-Octubre 2020.xlsx → Junio-Octubre 2020.csv (327 rows)\n",
      "✔ Marzo-Julio 2021.xlsx → Marzo-Julio 2021.csv (425 rows)\n",
      "✔ Mayo-Agosto 2022.xlsx → Mayo-Agosto 2022.csv (332 rows)\n",
      "✔ Mayo-Agosto 2023.xlsx → Mayo-Agosto 2023.csv (422 rows)\n",
      "✔ Noviembre-Febrero 2021.xlsx → Noviembre-Febrero 2021.csv (253 rows)\n",
      "✔ Septiembre-Diciembre 2022.xlsx → Septiembre-Diciembre 2022.csv (344 rows)\n",
      "✔ Septiembre-Diciembre 2023.xlsx → Septiembre-Diciembre 2023.csv (380 rows)\n",
      "✔ Septiembre-Noviembre 2024.xlsx → Septiembre-Noviembre 2024.csv (253 rows)\n",
      "✅ Converted 18 Excel files to CSV\n",
      "✔ Loaded Abril-Junio 2024.csv (334 rows)\n",
      "✔ Loaded Agosto-Diciembre 2021.csv (406 rows)\n",
      "✔ Loaded Diciembre-Agosto 2025.csv (506 rows)\n",
      "✔ Loaded Diciembre-Febrero 2025.csv (220 rows)\n",
      "✔ Loaded Enero - Abril 2023.csv (334 rows)\n",
      "✔ Loaded Enero-Abril 2022.csv (275 rows)\n",
      "✔ Loaded Enero-Marzo 2024.csv (294 rows)\n",
      "✔ Loaded Enero-Mayo 2020.csv (236 rows)\n",
      "✔ Loaded Julio-Agosto 2024.csv (353 rows)\n",
      "✔ Loaded Junio-Diciembre 2019.csv (349 rows)\n",
      "✔ Loaded Junio-Octubre 2020.csv (327 rows)\n",
      "✔ Loaded Marzo-Julio 2021.csv (425 rows)\n",
      "✔ Loaded Mayo-Agosto 2022.csv (332 rows)\n",
      "✔ Loaded Mayo-Agosto 2023.csv (422 rows)\n",
      "✔ Loaded Noviembre-Febrero 2021.csv (253 rows)\n",
      "✔ Loaded Septiembre-Diciembre 2022.csv (344 rows)\n",
      "✔ Loaded Septiembre-Diciembre 2023.csv (380 rows)\n",
      "✔ Loaded Septiembre-Noviembre 2024.csv (253 rows)\n",
      "⚠ Removed 94 duplicate rows\n",
      "⚠ Removed 48 duplicates using keys: ['fecha', 'concepto', 'importe', 'divisa']\n",
      "✅ Combined file saved to: c:\\Users\\diego\\Desktop\\Proyectos GitHub\\data\\processed\\combined.csv (5901 rows)\n",
      "✅ Combined CSV created with 5901 rows\n"
     ]
    }
   ],
   "source": [
    "# === Run pipeline: Excel -> CSV -> Combined ===\n",
    "\n",
    "# 1) Convert Excel files to intermediate CSVs\n",
    "converted_files = convert_excels(RAW_DIR, STAGE_CSV_DIR, skip_existing=True)\n",
    "\n",
    "if converted_files:\n",
    "    print(f\"✅ Converted {len(converted_files)} Excel files to CSV\")\n",
    "else:\n",
    "    print(\"⚠ No Excel files were converted\")\n",
    "\n",
    "# 2) Combine all intermediate CSVs into a single file\n",
    "df_combined = process_combined_files(STAGE_CSV_DIR, combined_csv_path)\n",
    "\n",
    "if df_combined is not None:\n",
    "    print(f\"✅ Combined CSV created with {len(df_combined)} rows\")\n",
    "else:\n",
    "    print(\"⚠ No combined CSV created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a1f05",
   "metadata": {},
   "source": [
    "#### Ejecutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8568edd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (5901, 10)\n",
      "Columns: ['fecha', 'concepto', 'movimiento', 'importe', 'divisa', 'disponible', 'observaciones', 'unnamed: 0', 'fvalor', 'divisa1']\n",
      "        fecha                     concepto  \\\n",
      "0  28/06/2024      Corp alim guissona t561   \n",
      "1  28/06/2024    Mercadona rambla del pobl   \n",
      "2  28/06/2024          Adeudo de carrefour   \n",
      "3  28/06/2024  Traspaso programa tu cuenta   \n",
      "4  27/06/2024                      Caprabo   \n",
      "\n",
      "                                       movimiento  importe divisa  disponible  \\\n",
      "0                                Pago con tarjeta   -15.40    EUR      854.25   \n",
      "1                                Pago con tarjeta   -22.47    EUR      869.65   \n",
      "2                      Adeudo nº 2024173000746549    -9.99    EUR      892.12   \n",
      "3  Trp redondeo tarjeta          4940197125842350    -0.13    EUR      902.11   \n",
      "4                                Pago con tarjeta    -2.99    EUR      902.24   \n",
      "\n",
      "                                       observaciones  unnamed: 0      fvalor  \\\n",
      "0  4940197125842350 CORP ALIM GUISSONA T561  BARC...         NaN  28/06/2024   \n",
      "1  4940197125842350 MERCADONA RAMBLA DEL POBLBARC...         NaN  28/06/2024   \n",
      "2  N 2024173000746549 SERVICIOS FINANCIEROS CARRE...         NaN  28/06/2024   \n",
      "3     TRP REDONDEO TARJETA          4940197125842350         NaN  28/06/2024   \n",
      "4  4940197125842350 CAPRABO                  BARC...         NaN  27/06/2024   \n",
      "\n",
      "  divisa1  \n",
      "0     EUR  \n",
      "1     EUR  \n",
      "2     EUR  \n",
      "3     EUR  \n",
      "4     EUR  \n"
     ]
    }
   ],
   "source": [
    "# Quick check of the combined file\n",
    "df = pd.read_csv(combined_csv_path)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7086292b",
   "metadata": {},
   "source": [
    "## Limpieza Inicial de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed23f78",
   "metadata": {},
   "source": [
    "### Limpieza inicial y exploracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb275311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Dropped junk columns: ['unnamed: 0', 'divisa1']\n",
      "ℹ Using 'fvalor' as the correct date and renaming it to 'fecha'.\n",
      "Shape: (5901, 7)\n",
      "Columns: ['concepto', 'movimiento', 'importe', 'divisa', 'disponible', 'observaciones', 'fecha']\n",
      "concepto                 object\n",
      "movimiento               object\n",
      "importe                 float64\n",
      "divisa                   object\n",
      "disponible              float64\n",
      "observaciones            object\n",
      "fecha            datetime64[ns]\n",
      "dtype: object\n",
      "                    concepto                  movimiento  importe divisa  \\\n",
      "0    Corp alim guissona t561            Pago con tarjeta   -15.40    EUR   \n",
      "1  Mercadona rambla del pobl            Pago con tarjeta   -22.47    EUR   \n",
      "2        Adeudo de carrefour  Adeudo nº 2024173000746549    -9.99    EUR   \n",
      "\n",
      "   disponible                                      observaciones      fecha  \n",
      "0      854.25  4940197125842350 CORP ALIM GUISSONA T561  BARC... 2024-06-28  \n",
      "1      869.65  4940197125842350 MERCADONA RAMBLA DEL POBLBARC... 2024-06-28  \n",
      "2      892.12  N 2024173000746549 SERVICIOS FINANCIEROS CARRE... 2024-06-28  \n"
     ]
    }
   ],
   "source": [
    "# Align columns to your actual combined.csv structure\n",
    "\n",
    "# 1) Drop junk columns safely\n",
    "junk_cols = []\n",
    "for c in df.columns:\n",
    "    if c.lower() in {\"divisa1\"}:\n",
    "        junk_cols.append(c)\n",
    "    if c.lower().startswith(\"unnamed\"):\n",
    "        junk_cols.append(c)\n",
    "\n",
    "if junk_cols:\n",
    "    df = df.drop(columns=junk_cols)\n",
    "    print(f\"🧹 Dropped junk columns: {junk_cols}\")\n",
    "\n",
    "# 2) Keep 'fvalor' as the true date -> rename to 'fecha' and drop any old 'fecha'\n",
    "has_fecha  = \"fecha\"  in df.columns\n",
    "has_fvalor = \"fvalor\" in df.columns\n",
    "\n",
    "if has_fvalor:\n",
    "    # If there is already a 'fecha', remove it because 'fvalor' is the correct one\n",
    "    if has_fecha:\n",
    "        df = df.drop(columns=[\"fecha\"])\n",
    "    df = df.rename(columns={\"fvalor\": \"fecha\"})\n",
    "    print(\"ℹ Using 'fvalor' as the correct date and renaming it to 'fecha'.\")\n",
    "elif not has_fecha:\n",
    "    print(\"⚠ Neither 'fvalor' nor 'fecha' found. Please check your inputs.\")\n",
    "\n",
    "# 3) Parse 'fecha' to datetime (ES format)\n",
    "if \"fecha\" in df.columns:\n",
    "    df[\"fecha\"] = pd.to_datetime(df[\"fecha\"], errors=\"coerce\", dayfirst=True)\n",
    "else:\n",
    "    print(\"⚠ No 'fecha' column present after adjustments. Check your inputs.\")\n",
    "\n",
    "# 4) Quick check\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.dtypes)\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "340e7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_substrings_from_column(df, column, substrings):\n",
    "    \"\"\"\n",
    "    Remove any of the given substrings from a text column.\n",
    "    - Builds ONE regex for all substrings (faster than looping)\n",
    "    - Safe with NaN\n",
    "    - Trims extra spaces after removal\n",
    "    \"\"\"\n",
    "    if column not in df.columns or not substrings:\n",
    "        return df\n",
    "\n",
    "    pattern = \"|\".join(re.escape(s) for s in substrings)\n",
    "    s = df[column].astype(str).fillna(\"\")\n",
    "\n",
    "    s = (\n",
    "        s.str.replace(pattern, \"\", regex=True)\n",
    "         .str.replace(r\"\\s{2,}\", \" \", regex=True)\n",
    "         .str.strip()\n",
    "    )\n",
    "    df[column] = s\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_rows_if_column_contains(df, column, substrings, case_insensitive=True, word_boundaries=False):\n",
    "    \"\"\"\n",
    "    Drop rows where 'column' contains ANY of the substrings.\n",
    "    - Uses a compiled regex (no capturing groups) to avoid pandas UserWarning.\n",
    "    \"\"\"\n",
    "    if column not in df.columns or not substrings:\n",
    "        return df\n",
    "\n",
    "    escaped = [re.escape(s) for s in substrings]\n",
    "    if word_boundaries:\n",
    "        parts = [rf\"\\b{e}\\b\" for e in escaped]\n",
    "    else:\n",
    "        parts = escaped\n",
    "\n",
    "    pattern = \"|\".join(parts)              # no () -> no capturing groups\n",
    "    flags = re.IGNORECASE if case_insensitive else 0\n",
    "    regex = re.compile(pattern, flags)     # compile once\n",
    "\n",
    "    mask_keep = ~df[column].astype(str).str.contains(regex, na=False)\n",
    "    return df.loc[mask_keep].reset_index(drop=True)\n",
    "\n",
    "    return df.loc[mask_keep].reset_index(drop=True)\n",
    "\n",
    "\n",
    "def clean_specific_fields(df,\n",
    "                          observations_numbers=None,\n",
    "                          movement_numbers=None,\n",
    "                          movements_to_remove=None,\n",
    "                          concepts_to_remove=None):\n",
    "    \"\"\"\n",
    "    Wrapper that applies your three steps in order:\n",
    "    1) Remove numbers from 'observaciones'\n",
    "    2) Remove numbers from 'movimiento'\n",
    "    3) Drop rows by 'movimiento' and 'concepto'\n",
    "    \"\"\"\n",
    "    observations_numbers = observations_numbers or []\n",
    "    movement_numbers     = movement_numbers or []\n",
    "    movements_to_remove  = movements_to_remove or []\n",
    "    concepts_to_remove   = concepts_to_remove or []\n",
    "\n",
    "    if \"observaciones\" in df.columns:\n",
    "        df = remove_substrings_from_column(df, \"observaciones\", observations_numbers)\n",
    "\n",
    "    if \"movimiento\" in df.columns:\n",
    "        df = remove_substrings_from_column(df, \"movimiento\", movement_numbers)\n",
    "\n",
    "    if \"movimiento\" in df.columns:\n",
    "        df = drop_rows_if_column_contains(df, \"movimiento\", movements_to_remove, case_insensitive=True)\n",
    "\n",
    "    if \"concepto\" in df.columns:\n",
    "        df = drop_rows_if_column_contains(df, \"concepto\", concepts_to_remove, case_insensitive=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "014c2e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Specific cleaning applied. Shape: (4907, 7)\n"
     ]
    }
   ],
   "source": [
    "observations_numbers = ['4940197125842350', '2024173000746549', '4543390531579234', '5181760018112648']\n",
    "movement_numbers     = ['2024173000746549']\n",
    "movements_to_remove  = ['Trp redondeo tarjeta']\n",
    "concepts_to_remove   = ['mes cuentas claras', 'Comision mensual cuentas claras']\n",
    "\n",
    "df = clean_specific_fields(\n",
    "    df,\n",
    "    observations_numbers=observations_numbers,\n",
    "    movement_numbers=movement_numbers,\n",
    "    movements_to_remove=movements_to_remove,\n",
    "    concepts_to_remove=concepts_to_remove\n",
    ")\n",
    "\n",
    "print(\"✅ Specific cleaning applied. Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b111fe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned file saved to: c:\\Users\\diego\\Desktop\\Proyectos GitHub\\data\\processed\\cleaned.csv (4907 rows)\n"
     ]
    }
   ],
   "source": [
    "# === Finalize cleaning and save cleaned.csv ===\n",
    "# Ensure numeric types\n",
    "for col in [\"importe\", \"disponible\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid date or amount\n",
    "before = len(df)\n",
    "df = df[df[\"fecha\"].notna()]\n",
    "df = df[df[\"importe\"].notna()]\n",
    "after = len(df)\n",
    "if after != before:\n",
    "    print(f\"⚠ Removed {before - after} rows with invalid date/amount\")\n",
    "\n",
    "# Normalize text columns (trim spaces)\n",
    "for col in [\"concepto\", \"movimiento\", \"observaciones\", \"divisa\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "# Derive date parts (optional, handy for Power BI)\n",
    "df[\"year\"]  = df[\"fecha\"].dt.year\n",
    "df[\"month\"] = df[\"fecha\"].dt.month\n",
    "df[\"ym\"]    = df[\"fecha\"].dt.to_period(\"M\").astype(str)  # e.g., '2024-06'\n",
    "df[\"weekday\"] = df[\"fecha\"].dt.day_name(locale=\"es_ES\") if hasattr(df[\"fecha\"].dt, \"day_name\") else df[\"fecha\"].dt.day_name()\n",
    "\n",
    "# Save cleaned file\n",
    "cleaned_csv_path = PROCESSED_DIR / \"cleaned.csv\"\n",
    "df.to_csv(cleaned_csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Cleaned file saved to: {cleaned_csv_path} ({len(df)} rows)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb145b7",
   "metadata": {},
   "source": [
    "## Categorizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9971135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(ch for ch in unicodedata.normalize(\"NFKD\", str(s)) if not unicodedata.combining(ch))\n",
    "\n",
    "def normalize_text_for_match(series):\n",
    "    s = series.astype(str).fillna(\"\")\n",
    "    s = s.str.lower().apply(strip_accents)\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    return s\n",
    "\n",
    "# Build a single column to match against (concept + observations)\n",
    "def build_match_text(df):\n",
    "    df[\"concepto_norm\"]      = normalize_text_for_match(df[\"concepto\"]) if \"concepto\" in df.columns else \"\"\n",
    "    df[\"observaciones_norm\"] = normalize_text_for_match(df[\"observaciones\"]) if \"observaciones\" in df.columns else \"\"\n",
    "    df[\"match_text\"] = (df.get(\"concepto_norm\", \"\") + \" \" + df.get(\"observaciones_norm\",\"\")).str.strip()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abfa9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_match_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd68e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_category_dict(csv_path):\n",
    "    \"\"\"\n",
    "    Load a two-column CSV mapping: Category, ConceptKeyword\n",
    "    Accepts files with or without header.\n",
    "    Returns a dict: {category: [keywords,...]}\n",
    "    \"\"\"\n",
    "    # try with header first\n",
    "    try:\n",
    "        df_dic = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "        if df_dic.shape[1] < 2:\n",
    "            raise ValueError(\"Dictionary file must have at least 2 columns\")\n",
    "        if not {\"Categoria\",\"Concepto\"}.issubset(set(df_dic.columns)):\n",
    "            # fallback: assume no header\n",
    "            df_dic = pd.read_csv(csv_path, header=None, encoding=\"utf-8-sig\")\n",
    "            df_dic = df_dic.rename(columns={0:\"Categoria\", 1:\"Concepto\"})\n",
    "    except Exception:\n",
    "        # hard fallback\n",
    "        df_dic = pd.read_csv(csv_path, header=None, encoding=\"utf-8-sig\")\n",
    "        df_dic = df_dic.rename(columns={0:\"Categoria\", 1:\"Concepto\"})\n",
    "\n",
    "    # normalize Concepto for matching\n",
    "    df_dic[\"Concepto\"] = normalize_text_for_match(df_dic[\"Concepto\"])\n",
    "    cat_dict = df_dic.groupby(\"Categoria\")[\"Concepto\"].apply(list).to_dict()\n",
    "    return cat_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b108cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Category dictionary loaded. Categories: ['amazon', 'bizum', 'comida', 'comida_fuera', 'compras', 'deporte', 'impuestos', 'mascota', 'movimiento', 'ocio', 'otros', 'prestamo', 'salud', 'seguros', 'servicios', 'suscripcion', 'tecnologia', 'transporte', 'viajes', 'vivienda']\n"
     ]
    }
   ],
   "source": [
    "cat_dict = load_category_dict(dictionary_path)\n",
    "print(\"✅ Category dictionary loaded. Categories:\", list(cat_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56e38daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_with_dict(df, cat_dict, target_col=\"category\", source_col=\"match_text\", priorities=None):\n",
    "    \"\"\"\n",
    "    Apply categories based on regex word matches over source_col.\n",
    "    - cat_dict: {category: [keywords]}\n",
    "    - priorities: optional dict {category: int}; lower = higher priority\n",
    "    \"\"\"\n",
    "    if source_col not in df.columns:\n",
    "        raise ValueError(f\"{source_col} not in DataFrame\")\n",
    "\n",
    "    # prepare priority\n",
    "    if priorities is None:\n",
    "        priorities = {cat: 1000 for cat in cat_dict}  # default low priority\n",
    "    # build regex per category\n",
    "    compiled = {}\n",
    "    for cat, words in cat_dict.items():\n",
    "        parts = [rf\"\\b{re.escape(w)}\\b\" for w in words if str(w).strip() != \"\"]\n",
    "        if not parts:\n",
    "            continue\n",
    "        regex = re.compile(\"|\".join(parts), flags=re.IGNORECASE)\n",
    "        compiled[cat] = regex\n",
    "\n",
    "    # initialize\n",
    "    df[target_col] = np.nan\n",
    "\n",
    "    # assign categories by priority\n",
    "    cats_sorted = sorted(compiled.keys(), key=lambda c: priorities.get(c, 1000))\n",
    "    for cat in cats_sorted:\n",
    "        rx = compiled[cat]\n",
    "        mask = df[target_col].isna() & df[source_col].str.contains(rx, na=False)\n",
    "        if mask.any():\n",
    "            df.loc[mask, target_col] = cat\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95e1f95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_6212\\1634456265.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'amazon' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[mask, target_col] = cat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Initial categorization done.\n"
     ]
    }
   ],
   "source": [
    "df = categorize_with_dict(df, cat_dict, target_col=\"category\", source_col=\"match_text\")\n",
    "print(\"✅ Initial categorization done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c43ff10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_correction_rules(df, rules_map, source_col=\"match_text\", target_col=\"category\"):\n",
    "    \"\"\"\n",
    "    Apply explicit mapping patterns: {pattern_string: category}\n",
    "    - Matching is case-insensitive and accent-insensitive due to 'match_text'\n",
    "    \"\"\"\n",
    "    compiled_rules = [(re.compile(re.escape(p), re.IGNORECASE), cat) for p, cat in rules_map.items()]\n",
    "\n",
    "    for rx, cat in compiled_rules:\n",
    "        mask = df[source_col].str.contains(rx, na=False)\n",
    "        if mask.any():\n",
    "            df.loc[mask, target_col] = cat\n",
    "    return df\n",
    "\n",
    "def force_income_for_positive_amounts(df, amount_col=\"importe\", target_col=\"category\", income_label=\"ingresos\"):\n",
    "    if amount_col in df.columns:\n",
    "        df.loc[df[amount_col] > 0, target_col] = income_label\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eaec6fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Corrections applied and positive amounts forced to 'ingresos'.\n"
     ]
    }
   ],
   "source": [
    "rules = {\n",
    "    \"pasaje turquia arianna\": \"viajes\",\n",
    "    \"ahorro\": \"ahorro\",\n",
    "    \"airbnb milan\": \"viajes\",\n",
    "    \"alquiler pallars\": \"vivienda\",\n",
    "    \"prohorta\": \"vivienda\",\n",
    "    \"dueñas\": \"salud\",\n",
    "    \"traspaso desde cuenta\": \"movimiento\",\n",
    "    \"abo. por traspaso desde tarj.de\": \"Correction\",\n",
    "    \"adeudo mensual de tarjeta\": \"Correction\",\n",
    "    \"operacion financiada con tarjeta\": \"Correction\",\n",
    "    \"0182-4383-99-0830116863\": \"Correction\",\n",
    "    \"01828740 999\": \"Correction\",\n",
    "    \"01820209 999\": \"Correction\",\n",
    "    \"prestamo lu/ari\": \"Correction\",\n",
    "    \"piso arianna\": \"Correction\",\n",
    "    \"diego antonio mere caravelli\": \"Correction\"\n",
    "}\n",
    "\n",
    "df = apply_correction_rules(df, rules, source_col=\"match_text\", target_col=\"category\")\n",
    "df = force_income_for_positive_amounts(df, amount_col=\"importe\", target_col=\"category\", income_label=\"ingresos\")\n",
    "print(\"✅ Corrections applied and positive amounts forced to 'ingresos'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75a089c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total transacciones sin categoría: 25\n",
      "Total conceptos únicos sin categoría: 10\n",
      "\n",
      "Top 20 conceptos sin categoría:\n",
      "concepto\n",
      "Nasae s.l.                  12\n",
      "El maracucho s.l.            3\n",
      "Boheme cafeteria.            2\n",
      "Yelmo films s.l.             2\n",
      "Cosi duci s.l.               1\n",
      "Locanda del vulture s.l.     1\n",
      "Distinta s.l.                1\n",
      "Cool partners s.l.           1\n",
      "Thekedar dhupsari s.l.       1\n",
      "Sushi malasaãa,s.l.          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "uncat = df[df[\"category\"].isna()].copy()\n",
    "if uncat.empty:\n",
    "    print(\"¡Genial! Todos los conceptos están categorizados.\")\n",
    "else:\n",
    "    freq = uncat[\"concepto\"].value_counts()\n",
    "    print(f\"Total transacciones sin categoría: {len(uncat)}\")\n",
    "    print(f\"Total conceptos únicos sin categoría: {len(freq)}\")\n",
    "    print(\"\\nTop 20 conceptos sin categoría:\")\n",
    "    print(freq.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa3085cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 10 new concepts to categorize.\n",
      "Updating dictionary with uncategorized concepts...\n",
      "Uncategorized concept: Boheme cafeteria.\n",
      "➡️ Exiting manual categorization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\diego\\AppData\\Local\\Temp\\ipykernel_6212\\1634456265.py:31: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'amazon' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[mask, target_col] = cat\n"
     ]
    }
   ],
   "source": [
    "def update_dictionary_interactive(cat_dict, concepts, number_to_category):\n",
    "    \"\"\"\n",
    "    Interactive mapping: prompts the user to assign categories to unknown concepts.\n",
    "    Press 'q' to quit.\n",
    "    \"\"\"\n",
    "    for concept in concepts:\n",
    "        print(\"Uncategorized concept:\", concept)\n",
    "        choice = input(\"Enter category number (or 'q' to quit): \").strip()\n",
    "        if choice.lower() in {\"q\", \"quit\", \"salir\"}:\n",
    "            print(\"➡️ Exiting manual categorization.\")\n",
    "            break\n",
    "        if choice in number_to_category:\n",
    "            cat = number_to_category[choice]\n",
    "            cat_dict.setdefault(cat, []).append(concept)\n",
    "        else:\n",
    "            print(\"Invalid category number. Try again.\")\n",
    "    return cat_dict\n",
    "\n",
    "# mapping numbers -> category names (as you had)\n",
    "number_to_category = {\n",
    "    '1': 'comida_fuera','2': 'suscripcion','3': 'vivienda','4': 'ocio','5': 'transporte',\n",
    "    '6': 'impuestos','7': 'viajes','8': 'mascota','9': 'seguros','10': 'deporte',\n",
    "    '11': 'prestamo','12': 'compras','13': 'servicios','14': 'salud','15': 'tecnologia',\n",
    "    '16': 'comida','17': 'bizum','18': 'amazon','19': 'otros'\n",
    "}\n",
    "\n",
    "unknown_concepts = sorted(set(uncat[\"concepto\"].tolist())) if not uncat.empty else []\n",
    "print(f\"✅ Found {len(unknown_concepts)} new concepts to categorize.\")\n",
    "if unknown_concepts:\n",
    "    print(\"Updating dictionary with uncategorized concepts...\")\n",
    "    cat_dict = update_dictionary_interactive(cat_dict, unknown_concepts, number_to_category)\n",
    "    # Rebuild normalized match text for the updated keywords\n",
    "    # (ensure all dict keywords are normalized for matching)\n",
    "    cat_dict = {k: [strip_accents(str(x)).lower().strip() for x in v] for k,v in cat_dict.items()}\n",
    "    # Re-apply categorization\n",
    "    df = categorize_with_dict(df, cat_dict, target_col=\"category\", source_col=\"match_text\")\n",
    "    df = apply_correction_rules(df, rules, source_col=\"match_text\", target_col=\"category\")\n",
    "    df = force_income_for_positive_amounts(df, amount_col=\"importe\", target_col=\"category\", income_label=\"ingresos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f38522df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Category dictionary updated on disk.\n"
     ]
    }
   ],
   "source": [
    "def save_category_dict(cat_dict, csv_path):\n",
    "    rows = []\n",
    "    for cat, words in cat_dict.items():\n",
    "        for w in words:\n",
    "            rows.append({\"Categoria\": cat, \"Concepto\": w})\n",
    "    out = pd.DataFrame(rows)\n",
    "    out.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# save back\n",
    "save_category_dict(cat_dict, dictionary_path)\n",
    "print(\"✅ Category dictionary updated on disk.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98fc34e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Categorized file saved to: c:\\Users\\diego\\Desktop\\Proyectos GitHub\\data\\processed\\categorized.csv (4907 rows)\n"
     ]
    }
   ],
   "source": [
    "final_cols = [c for c in [\"fecha\",\"importe\",\"category\",\"concepto\",\"observaciones\"] if c in df.columns]\n",
    "final_df = df[final_cols].copy()\n",
    "\n",
    "final_output_path = PROCESSED_DIR / \"categorized.csv\"\n",
    "save_as_csv(final_df, final_output_path)\n",
    "print(f\"✅ Categorized file saved to: {final_output_path} ({len(final_df)} rows)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
